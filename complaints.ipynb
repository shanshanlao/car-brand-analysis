{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shans\\AppData\\Local\\Temp\\ipykernel_26196\\2409493507.py:2: DtypeWarning: Columns (9,16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('complaints.csv')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-product</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>Company public response</th>\n",
       "      <th>Company</th>\n",
       "      <th>State</th>\n",
       "      <th>ZIP code</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Consumer consent provided?</th>\n",
       "      <th>Submitted via</th>\n",
       "      <th>Date sent to company</th>\n",
       "      <th>Company response to consumer</th>\n",
       "      <th>Timely response?</th>\n",
       "      <th>Consumer disputed?</th>\n",
       "      <th>Complaint ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Improper use of your report</td>\n",
       "      <td>Credit inquiries on your report that you don't...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQUIFAX, INC.</td>\n",
       "      <td>AL</td>\n",
       "      <td>36736.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-01-10</td>\n",
       "      <td>In progress</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6414057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>Checking or savings account</td>\n",
       "      <td>Checking account</td>\n",
       "      <td>Managing an account</td>\n",
       "      <td>Cashing a check</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FIFTH THIRD FINANCIAL CORPORATION</td>\n",
       "      <td>OH</td>\n",
       "      <td>45208.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Phone</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6416102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>I do not know</td>\n",
       "      <td>Attempts to collect debt not owed</td>\n",
       "      <td>Debt is not yours</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bull City Financial Solutions, Inc</td>\n",
       "      <td>NC</td>\n",
       "      <td>27858.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6415994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Problem with a credit reporting company's inve...</td>\n",
       "      <td>Difficulty submitting a dispute or getting inf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQUIFAX, INC.</td>\n",
       "      <td>TX</td>\n",
       "      <td>76002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>In progress</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6418427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Problem with a credit reporting company's inve...</td>\n",
       "      <td>Difficulty submitting a dispute or getting inf...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EQUIFAX, INC.</td>\n",
       "      <td>TX</td>\n",
       "      <td>76002.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>In progress</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6418451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Date received                                            Product  \\\n",
       "0    2023-01-10  Credit reporting, credit repair services, or o...   \n",
       "1    2023-01-09                        Checking or savings account   \n",
       "2    2023-01-09                                    Debt collection   \n",
       "3    2023-01-09  Credit reporting, credit repair services, or o...   \n",
       "4    2023-01-09  Credit reporting, credit repair services, or o...   \n",
       "\n",
       "        Sub-product                                              Issue  \\\n",
       "0  Credit reporting                        Improper use of your report   \n",
       "1  Checking account                                Managing an account   \n",
       "2     I do not know                  Attempts to collect debt not owed   \n",
       "3  Credit reporting  Problem with a credit reporting company's inve...   \n",
       "4  Credit reporting  Problem with a credit reporting company's inve...   \n",
       "\n",
       "                                           Sub-issue  \\\n",
       "0  Credit inquiries on your report that you don't...   \n",
       "1                                    Cashing a check   \n",
       "2                                  Debt is not yours   \n",
       "3  Difficulty submitting a dispute or getting inf...   \n",
       "4  Difficulty submitting a dispute or getting inf...   \n",
       "\n",
       "  Consumer complaint narrative Company public response  \\\n",
       "0                          NaN                     NaN   \n",
       "1                          NaN                     NaN   \n",
       "2                          NaN                     NaN   \n",
       "3                          NaN                     NaN   \n",
       "4                          NaN                     NaN   \n",
       "\n",
       "                              Company State ZIP code Tags  \\\n",
       "0                       EQUIFAX, INC.    AL  36736.0  NaN   \n",
       "1   FIFTH THIRD FINANCIAL CORPORATION    OH  45208.0  NaN   \n",
       "2  Bull City Financial Solutions, Inc    NC  27858.0  NaN   \n",
       "3                       EQUIFAX, INC.    TX  76002.0  NaN   \n",
       "4                       EQUIFAX, INC.    TX  76002.0  NaN   \n",
       "\n",
       "  Consumer consent provided? Submitted via Date sent to company  \\\n",
       "0                      Other           Web           2023-01-10   \n",
       "1                        NaN         Phone           2023-01-09   \n",
       "2                        NaN           Web           2023-01-09   \n",
       "3                        NaN           Web           2023-01-09   \n",
       "4                        NaN           Web           2023-01-09   \n",
       "\n",
       "  Company response to consumer Timely response? Consumer disputed?  \\\n",
       "0                  In progress              Yes                NaN   \n",
       "1      Closed with explanation              Yes                NaN   \n",
       "2      Closed with explanation              Yes                NaN   \n",
       "3                  In progress              Yes                NaN   \n",
       "4                  In progress              Yes                NaN   \n",
       "\n",
       "   Complaint ID  \n",
       "0       6414057  \n",
       "1       6416102  \n",
       "2       6415994  \n",
       "3       6418427  \n",
       "4       6418451  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data\n",
    "df = pd.read_csv('complaints.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3244309, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Products and Consumer Complaint narrative\n",
    "new_df = df[['Product','Consumer complaint narrative']]\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2072247"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['Consumer complaint narrative'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1172062, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove NA values\n",
    "new_df = new_df.dropna()\n",
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>This debt collector company by name, Credit Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>We were a victim of Hurricane Ian. My wife and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>I sent a letter to I.C. Systems on XX/XX/2022,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>I have been a member with USAA Federal Savings...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>I bought my leased vehicle from XXXX XXXX in X...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                       Consumer complaint narrative\n",
       "40         Debt collection  This debt collector company by name, Credit Ma...\n",
       "63                Mortgage  We were a victim of Hurricane Ian. My wife and...\n",
       "159        Debt collection  I sent a letter to I.C. Systems on XX/XX/2022,...\n",
       "185        Debt collection  I have been a member with USAA Federal Savings...\n",
       "467  Vehicle loan or lease  I bought my leased vehicle from XXXX XXXX in X..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of columns with multiple product lables\n",
    "new_df = new_df[new_df['Product'].str.count(',') == 0]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing purpose, deal with the first 100 rows first\n",
    "new_df = new_df.iloc[:1000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[this, debt, collector, company, by, name, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>[we, were, a, victim, of, hurricane, ian, ., m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>[i, sent, a, letter, to, i.c, ., systems, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>[i, have, been, a, member, with, usaa, federal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>[i, bought, my, leased, vehicle, from, xxxx, x...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tokenized\n",
       "40   [this, debt, collector, company, by, name, ,, ...\n",
       "63   [we, were, a, victim, of, hurricane, ian, ., m...\n",
       "159  [i, sent, a, letter, to, i.c, ., systems, on, ...\n",
       "185  [i, have, been, a, member, with, usaa, federal...\n",
       "467  [i, bought, my, leased, vehicle, from, xxxx, x..."
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get part-of-speech tags for each token in a complaint\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "new_df['tokenized'] = new_df['Consumer complaint narrative'].str.lower().apply(nltk.word_tokenize)\n",
    "new_df[['tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[(this, DT), (debt, NN), (collector, NN), (com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>[(we, PRP), (were, VBD), (a, DT), (victim, NN)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>[(i, NN), (sent, VBD), (a, DT), (letter, NN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>[(i, NNS), (have, VBP), (been, VBN), (a, DT), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>[(i, NN), (bought, VBD), (my, PRP$), (leased, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tagged\n",
       "40   [(this, DT), (debt, NN), (collector, NN), (com...\n",
       "63   [(we, PRP), (were, VBD), (a, DT), (victim, NN)...\n",
       "159  [(i, NN), (sent, VBD), (a, DT), (letter, NN), ...\n",
       "185  [(i, NNS), (have, VBP), (been, VBN), (a, DT), ...\n",
       "467  [(i, NN), (bought, VBD), (my, PRP$), (leased, ..."
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Move this to the end\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "new_df['tagged'] = new_df['tokenized'].apply(nltk.pos_tag)\n",
    "new_df[['tagged']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all punctuation\n",
    "import string\n",
    "regular_punct = list(string.punctuation)\n",
    "\n",
    "# text as the row\n",
    "def remove_punctuation(text,punct_list):\n",
    "    for punc in punct_list:\n",
    "        if punc in text:\n",
    "            text = list(map(lambda x: x.replace(punc,''),text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40     [this, debt, collector, company, by, name, , c...\n",
       "63     [we, were, a, victim, of, hurricane, ian, , my...\n",
       "159    [i, sent, a, letter, to, ic, , systems, on, xx...\n",
       "185    [i, have, been, a, member, with, usaa, federal...\n",
       "467    [i, bought, my, leased, vehicle, from, xxxx, x...\n",
       "Name: punct_removed, dtype: object"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['punct_removed'] = [remove_punctuation(w,regular_punct) for w in new_df['tokenized']]\n",
    "new_df['punct_removed'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Query as the row \n",
    "def remove_stopwords(query):\n",
    "    result = [word for word in query if word not in stop_words]\n",
    "    #result = ' '.join(resultwords)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40     [debt, collector, company, name, , credit, man...\n",
       "63     [victim, hurricane, ian, , wife, took, time, g...\n",
       "159    [sent, letter, ic, , systems, xx/xx/2022, , co...\n",
       "185    [member, usaa, federal, savings, bank, 25, yea...\n",
       "467    [bought, leased, vehicle, xxxx, xxxx, xx/xx/20...\n",
       "Name: stop_words_removed, dtype: object"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['stop_words_removed'] = [remove_stopwords(w) for w in new_df['punct_removed']]\n",
    "#[w for w in new_df['punct_removed'] if w not in stop_words]\n",
    "new_df['stop_words_removed'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[(debt, NN), (collector, NN), (company, NN), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>[(victim, NN), (hurricane, NN), (ian, JJ), (, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>[(sent, JJ), (letter, NN), (ic, NN), (, NNP), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>[(member, NN), (usaa, JJ), (federal, JJ), (sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>[(bought, NN), (leased, VBD), (vehicle, NN), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           test_tagged\n",
       "40   [(debt, NN), (collector, NN), (company, NN), (...\n",
       "63   [(victim, NN), (hurricane, NN), (ian, JJ), (, ...\n",
       "159  [(sent, JJ), (letter, NN), (ic, NN), (, NNP), ...\n",
       "185  [(member, NN), (usaa, JJ), (federal, JJ), (sav...\n",
       "467  [(bought, NN), (leased, VBD), (vehicle, NN), (..."
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "new_df['test_tagged'] = new_df['stop_words_removed'].apply(nltk.pos_tag)\n",
    "new_df[['test_tagged']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# where treebank_tag is the second element in the tuple of ['tagged']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatizer.lemmatize('owe', get_wordnet_pos('PRP'))\n",
    "get_wordnet_pos('PRP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this DT\n",
      "debt NN\n",
      "collector NN\n",
      "company NN\n",
      "by IN\n",
      "name NN\n",
      ", ,\n",
      "credit NN\n",
      "management NN\n",
      "lp NN\n",
      "is VBZ\n",
      "falsely RB\n",
      "representing VBG\n",
      "that IN\n",
      "i JJ\n",
      "owe VBP\n",
      "the DT\n",
      "sum NN\n",
      "of IN\n",
      "{ (\n",
      "$ $\n",
      "59.00 CD\n",
      "} )\n",
      "to TO\n",
      "xxxx VB\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxxxxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      ". .\n",
      "i NN\n",
      "have VBP\n",
      "never RB\n",
      "come VBN\n",
      "across IN\n",
      "the DT\n",
      "name NN\n",
      "of IN\n",
      "xxxx NNP\n",
      ", ,\n",
      "xxxxxxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      ", ,\n",
      "let VB\n",
      "alone JJ\n",
      "order NN\n",
      "or CC\n",
      "request VB\n",
      "any DT\n",
      "service NN\n",
      "or CC\n",
      "product NN\n",
      "from IN\n",
      "them PRP\n",
      ". .\n",
      "i NN\n",
      "have VBP\n",
      "also RB\n",
      "never RB\n",
      "received VBD\n",
      "any DT\n",
      "bill NN\n",
      "from IN\n",
      "this DT\n",
      "xxxx JJ\n",
      "xxxxxxxx NNP\n",
      "xxxx NNP\n",
      "xxxxxxxx NNP\n",
      "xxxx NNP\n",
      ". .\n",
      "i NN\n",
      "received VBD\n",
      "only RB\n",
      "one CD\n",
      "debt NN\n",
      "collection NN\n",
      "notice NN\n",
      "from IN\n",
      "credit NN\n",
      "management NN\n",
      "lp NN\n",
      "and CC\n",
      "this DT\n",
      "was VBD\n",
      "dated VBN\n",
      "xx/xx/22 NNP\n",
      ". .\n",
      "i NN\n",
      "responded VBD\n",
      "to TO\n",
      "them PRP\n",
      "asking VBG\n",
      "for IN\n",
      "details NNS\n",
      "of IN\n",
      "this DT\n",
      "unknown JJ\n",
      "debt NN\n",
      "and CC\n",
      "requested VBN\n",
      "verification NN\n",
      "and CC\n",
      "validation NN\n",
      "amongst RB\n",
      "other JJ\n",
      "details NNS\n",
      ". .\n",
      "credit NN\n",
      "management NN\n",
      "lp NN\n",
      "responded VBD\n",
      "with IN\n",
      "only RB\n",
      "the DT\n",
      "same JJ\n",
      "information NN\n",
      "they PRP\n",
      "alluded VBD\n",
      "to TO\n",
      "in IN\n",
      "the DT\n",
      "only JJ\n",
      "other JJ\n",
      "letter NN\n",
      "they PRP\n",
      "sent VBD\n",
      "to TO\n",
      "me PRP\n",
      "earlier JJR\n",
      ", ,\n",
      "to TO\n",
      "the DT\n",
      "effect NN\n",
      "that IN\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "is VBZ\n",
      "the DT\n",
      "creditor NN\n",
      "and CC\n",
      "the DT\n",
      "only RB\n",
      "other JJ\n",
      "information NN\n",
      "they PRP\n",
      "provided VBD\n",
      "apart RB\n",
      "from IN\n",
      "the DT\n",
      "name NN\n",
      "of IN\n",
      "the DT\n",
      "creditor NN\n",
      "is VBZ\n",
      "the DT\n",
      "po NN\n",
      "box NN\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      ". .\n",
      "they PRP\n",
      "incredulously RB\n",
      "stated VBD\n",
      "that IN\n",
      "they PRP\n",
      "are VBP\n",
      "referring VBG\n",
      "me PRP\n",
      "back RB\n",
      "to TO\n",
      "the DT\n",
      "validation NN\n",
      "information NN\n",
      "they PRP\n",
      "provided VBD\n",
      "in IN\n",
      "the DT\n",
      "same JJ\n",
      "letter NN\n",
      "they PRP\n",
      "sent VBD\n",
      "earlier RB\n",
      "dated VBN\n",
      "xx/xx/22 NN\n",
      ". .\n",
      "this DT\n",
      "so RB\n",
      "called JJ\n",
      "validation NN\n",
      "information NN\n",
      "they PRP\n",
      "are VBP\n",
      "referring VBG\n",
      "to TO\n",
      "only RB\n",
      "provided VB\n",
      "the DT\n",
      "name NN\n",
      "of IN\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      "xxxx NNP\n",
      ", ,\n",
      "and CC\n",
      "the DT\n",
      "amount NN\n",
      "owed VBD\n",
      "{ (\n",
      "$ $\n",
      "59.00 CD\n",
      "} )\n",
      "as IN\n",
      "of IN\n",
      "xx/xx/22 NN\n",
      ". .\n",
      "as IN\n",
      "if IN\n",
      "the DT\n",
      "foregoing NN\n",
      "is VBZ\n",
      "not RB\n",
      "enough RB\n",
      ", ,\n",
      "credit NN\n",
      "mangement NN\n",
      "lp NN\n",
      ". .\n",
      "have VBP\n",
      "subsequently RB\n",
      "sent VBN\n",
      "this DT\n",
      "false JJ\n",
      "debt NN\n",
      "information NN\n",
      "to TO\n",
      "the DT\n",
      "credit NN\n",
      "bureaus NN\n",
      ", ,\n",
      "without IN\n",
      "providing VBG\n",
      "any DT\n",
      "validation NN\n",
      "of IN\n",
      "the DT\n",
      "debt NN\n",
      ". .\n",
      "credit NN\n",
      "managment NN\n",
      "lp NN\n",
      "indicated VBN\n",
      "in IN\n",
      "their PRP$\n",
      "only JJ\n",
      "response NN\n",
      "letter NN\n",
      ", ,\n",
      "that IN\n",
      "if IN\n",
      "i JJ\n",
      "sent VBD\n",
      "a DT\n",
      "written VBN\n",
      "requent NN\n",
      "to TO\n",
      "them PRP\n",
      "asking VBG\n",
      "for IN\n",
      "validation NN\n",
      "of IN\n",
      "the DT\n",
      "debt NN\n",
      ", ,\n",
      "they PRP\n",
      "will MD\n",
      "be VB\n",
      "sending VBG\n",
      "follow VB\n",
      "up RB\n",
      "correspondence NN\n",
      "to TO\n",
      "me PRP\n",
      ". .\n",
      "as IN\n",
      "of IN\n",
      "today NN\n",
      ", ,\n",
      "xx/xx/22 VBZ\n",
      "no DT\n",
      "correspondence NN\n",
      "have VBP\n",
      "been VBN\n",
      "received VBN\n",
      ", ,\n",
      "and CC\n",
      "yet RB\n",
      "they PRP\n",
      "have VBP\n",
      "rushed VBN\n",
      "the DT\n",
      "so RB\n",
      "called JJ\n",
      "debt NN\n",
      "to TO\n",
      "the DT\n",
      "credit NN\n",
      "bureaus NN\n",
      "! .\n",
      "! .\n",
      "! .\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for (token,tag) in new_df.iloc[0]['tagged']:\n",
    "    print(token,tag)\n",
    "#    print(lemmatizer.lemmatize(token, get_wordnet_pos(tag)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39m#return [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for (token, tag) in text]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m---> 14\u001b[0m test \u001b[39m=\u001b[39m [lemmatize_text(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m new_df[\u001b[39m'\u001b[39m\u001b[39mtest_tagged\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     15\u001b[0m test\n\u001b[0;32m     17\u001b[0m \u001b[39m#new_df['text_lemmatized'] = new_df['stop_words_removed'].apply(lemmatize_text)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[154], line 14\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[39m#return [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for (token, tag) in text]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m---> 14\u001b[0m test \u001b[39m=\u001b[39m [lemmatize_text(w) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m new_df[\u001b[39m'\u001b[39m\u001b[39mtest_tagged\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     15\u001b[0m test\n\u001b[0;32m     17\u001b[0m \u001b[39m#new_df['text_lemmatized'] = new_df['stop_words_removed'].apply(lemmatize_text)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[154], line 9\u001b[0m, in \u001b[0;36mlemmatize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      7\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m (token,tag) \u001b[39min\u001b[39;00m text: \u001b[39m#new_df.iloc[1]['test_tagged']:\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     result\u001b[39m.\u001b[39mappend(lemmatizer\u001b[39m.\u001b[39;49mlemmatize(token, get_wordnet_pos(tag)))\n\u001b[0;32m     11\u001b[0m \u001b[39m#return [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for (token, tag) in text]\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlemmatize\u001b[39m(\u001b[39mself\u001b[39m, word: \u001b[39mstr\u001b[39m, pos: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mn\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[39m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[39m=\u001b[39m wn\u001b[39m.\u001b[39;49m_morphy(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmin\u001b[39m(lemmas, key\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m) \u001b[39mif\u001b[39;00m lemmas \u001b[39melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2072\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m   2064\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_morphy\u001b[39m(\u001b[39mself\u001b[39m, form, pos, check_exceptions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   2065\u001b[0m     \u001b[39m# from jordanbg:\u001b[39;00m\n\u001b[0;32m   2066\u001b[0m     \u001b[39m# Given an original string x\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2069\u001b[0m     \u001b[39m# 3. If there are no matches, keep applying rules until you either\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m     \u001b[39m#    find a match or you can't go any further\u001b[39;00m\n\u001b[1;32m-> 2072\u001b[0m     exceptions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exception_map[pos]\n\u001b[0;32m   2073\u001b[0m     substitutions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMORPHOLOGICAL_SUBSTITUTIONS[pos]\n\u001b[0;32m   2075\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply_rules\u001b[39m(forms):\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "# Lemmatize all the tokens based on the POS tags you created\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# text as the row\n",
    "def lemmatize_text(text):\n",
    "    result = []\n",
    "    for (token,tag) in text: #new_df.iloc[1]['test_tagged']:\n",
    "        result.append(lemmatizer.lemmatize(token, get_wordnet_pos(tag)))\n",
    "\n",
    "    #return [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for (token, tag) in text]\n",
    "    return result\n",
    "    \n",
    "test = [lemmatize_text(w) for w in new_df['test_tagged']]\n",
    "test\n",
    "    \n",
    "#new_df['text_lemmatized'] = new_df['stop_words_removed'].apply(lemmatize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [lemmatizer.lemmatize(token,get_wordnet_pos(tag)) for (token,tag) in tokenize_pos(i) and set(list(token)) != {'x'}]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>tagged</th>\n",
       "      <th>punct_removed</th>\n",
       "      <th>stop_words_removed</th>\n",
       "      <th>text_lemmatized</th>\n",
       "      <th>test_tagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>This debt collector company by name, Credit Ma...</td>\n",
       "      <td>[this, debt, collector, company, by, name, ,, ...</td>\n",
       "      <td>[(this, DT), (debt, NN), (collector, NN), (com...</td>\n",
       "      <td>[this, debt, collector, company, by, name, , c...</td>\n",
       "      <td>[debt, collector, company, name, , credit, man...</td>\n",
       "      <td>[debt, collector, company, name, , credit, man...</td>\n",
       "      <td>[(debt, NN), (collector, NN), (company, NN), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>We were a victim of Hurricane Ian. My wife and...</td>\n",
       "      <td>[we, were, a, victim, of, hurricane, ian, ., m...</td>\n",
       "      <td>[(we, PRP), (were, VBD), (a, DT), (victim, NN)...</td>\n",
       "      <td>[we, were, a, victim, of, hurricane, ian, , my...</td>\n",
       "      <td>[victim, hurricane, ian, , wife, took, time, g...</td>\n",
       "      <td>[victim, hurricane, ian, , wife, took, time, g...</td>\n",
       "      <td>[(victim, NN), (hurricane, NN), (ian, JJ), (, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>I sent a letter to I.C. Systems on XX/XX/2022,...</td>\n",
       "      <td>[i, sent, a, letter, to, i.c, ., systems, on, ...</td>\n",
       "      <td>[(i, NN), (sent, VBD), (a, DT), (letter, NN), ...</td>\n",
       "      <td>[i, sent, a, letter, to, ic, , systems, on, xx...</td>\n",
       "      <td>[sent, letter, ic, , systems, xx/xx/2022, , co...</td>\n",
       "      <td>[sent, letter, ic, , system, xx/xx/2022, , col...</td>\n",
       "      <td>[(sent, JJ), (letter, NN), (ic, NN), (, NNP), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>I have been a member with USAA Federal Savings...</td>\n",
       "      <td>[i, have, been, a, member, with, usaa, federal...</td>\n",
       "      <td>[(i, NNS), (have, VBP), (been, VBN), (a, DT), ...</td>\n",
       "      <td>[i, have, been, a, member, with, usaa, federal...</td>\n",
       "      <td>[member, usaa, federal, savings, bank, 25, yea...</td>\n",
       "      <td>[member, usaa, federal, saving, bank, 25, year...</td>\n",
       "      <td>[(member, NN), (usaa, JJ), (federal, JJ), (sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>Vehicle loan or lease</td>\n",
       "      <td>I bought my leased vehicle from XXXX XXXX in X...</td>\n",
       "      <td>[i, bought, my, leased, vehicle, from, xxxx, x...</td>\n",
       "      <td>[(i, NN), (bought, VBD), (my, PRP$), (leased, ...</td>\n",
       "      <td>[i, bought, my, leased, vehicle, from, xxxx, x...</td>\n",
       "      <td>[bought, leased, vehicle, xxxx, xxxx, xx/xx/20...</td>\n",
       "      <td>[bought, leased, vehicle, xxxx, xxxx, xx/xx/20...</td>\n",
       "      <td>[(bought, NN), (leased, VBD), (vehicle, NN), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                       Consumer complaint narrative  \\\n",
       "40         Debt collection  This debt collector company by name, Credit Ma...   \n",
       "63                Mortgage  We were a victim of Hurricane Ian. My wife and...   \n",
       "159        Debt collection  I sent a letter to I.C. Systems on XX/XX/2022,...   \n",
       "185        Debt collection  I have been a member with USAA Federal Savings...   \n",
       "467  Vehicle loan or lease  I bought my leased vehicle from XXXX XXXX in X...   \n",
       "\n",
       "                                             tokenized  \\\n",
       "40   [this, debt, collector, company, by, name, ,, ...   \n",
       "63   [we, were, a, victim, of, hurricane, ian, ., m...   \n",
       "159  [i, sent, a, letter, to, i.c, ., systems, on, ...   \n",
       "185  [i, have, been, a, member, with, usaa, federal...   \n",
       "467  [i, bought, my, leased, vehicle, from, xxxx, x...   \n",
       "\n",
       "                                                tagged  \\\n",
       "40   [(this, DT), (debt, NN), (collector, NN), (com...   \n",
       "63   [(we, PRP), (were, VBD), (a, DT), (victim, NN)...   \n",
       "159  [(i, NN), (sent, VBD), (a, DT), (letter, NN), ...   \n",
       "185  [(i, NNS), (have, VBP), (been, VBN), (a, DT), ...   \n",
       "467  [(i, NN), (bought, VBD), (my, PRP$), (leased, ...   \n",
       "\n",
       "                                         punct_removed  \\\n",
       "40   [this, debt, collector, company, by, name, , c...   \n",
       "63   [we, were, a, victim, of, hurricane, ian, , my...   \n",
       "159  [i, sent, a, letter, to, ic, , systems, on, xx...   \n",
       "185  [i, have, been, a, member, with, usaa, federal...   \n",
       "467  [i, bought, my, leased, vehicle, from, xxxx, x...   \n",
       "\n",
       "                                    stop_words_removed  \\\n",
       "40   [debt, collector, company, name, , credit, man...   \n",
       "63   [victim, hurricane, ian, , wife, took, time, g...   \n",
       "159  [sent, letter, ic, , systems, xx/xx/2022, , co...   \n",
       "185  [member, usaa, federal, savings, bank, 25, yea...   \n",
       "467  [bought, leased, vehicle, xxxx, xxxx, xx/xx/20...   \n",
       "\n",
       "                                       text_lemmatized  \\\n",
       "40   [debt, collector, company, name, , credit, man...   \n",
       "63   [victim, hurricane, ian, , wife, took, time, g...   \n",
       "159  [sent, letter, ic, , system, xx/xx/2022, , col...   \n",
       "185  [member, usaa, federal, saving, bank, 25, year...   \n",
       "467  [bought, leased, vehicle, xxxx, xxxx, xx/xx/20...   \n",
       "\n",
       "                                           test_tagged  \n",
       "40   [(debt, NN), (collector, NN), (company, NN), (...  \n",
       "63   [(victim, NN), (hurricane, NN), (ian, JJ), (, ...  \n",
       "159  [(sent, JJ), (letter, NN), (ic, NN), (, NNP), ...  \n",
       "185  [(member, NN), (usaa, JJ), (federal, JJ), (sav...  \n",
       "467  [(bought, NN), (leased, VBD), (vehicle, NN), (...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb8d3085d426e5e8dee3252ffbadecffc2a8c816985f9d79249a1de42c265c0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
